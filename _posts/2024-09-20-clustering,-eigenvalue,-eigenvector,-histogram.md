---
#layout: post
title: 데이터 위클리 페이퍼 6 - 클러스터링, 고유값과 고유벡터, 히스토그램
date: 2024-09-20
description: # 검색어 및 글요약
categories: [Data_analysis, Weekly]        # 메인 카테고리, 하위 카테고리(생략가능)
tags:           # 반드시 소문자로 작성, 한글가능
- clustering
- histogram
#pin: true # 해당글을 홈에서 고정시킬지
#toc: false # 오른쪽 목차 설정
#comments: false # 댓글 설정

#image: # 미리보기 이미지 설정
#  path: /path/to/image # 경로
#  alt: image alternative text # 이미지 설명 (생략가능)

#mermaid: true # 다이어그램 생성 도구 (https://github.com/mermaid-js/mermaid)
#math : true # 수학도구
---


## 데이터 간의 유사도를 계산할 때, feature의 수가 많다면(예: 100개 이상), 이러한 high-dimensional clustering 문제를 해결하기 위한 방법들을 설명해 주세요
---

> 데이터에 feature(변수)의 수가 많아지면, 데이터의 수가 차원의 수(변수)보다 적어져 데이터 내부의 노이즈가 증가하여 모델의 성능이 저하되고 데이터 간 유사성을 파악하기 힘들어지는 '차원의 저주'가 발생할 수 있다.
> 차원의 저주를 방지하기 위해 '차원 축소'(Dimensionality Reduction) 기법을 사용한다.


- **차원 축소(Dimensionality Reduction)**
  - 차원 축소는 데이터에 있는 차원(변수)을 축소하여 정보 손실을 최소화하면서도 계산의 효율성을 높이는 방법이다. 대표적인 차원축소기법에는 `주성분 분석(PCA)`이 있다.
  - 차원 축소를 통해 차원의 저주와 과적합을 방지하고, 모형 복잡도가 감소하여 해석력이 확보되어 시각화 하기에도 용이하다.
  - 이미지나 음성, 영상과 같은 고차원 데이터가 증가함에 따라 차원 축소 기법의 필요성이 증가하고있다.

- **차원 축소 기법**
  - 주성분분석(PCA) : 
    - 상관관계가 높은 데이터의 선형 결합을 통해 고차원의 데이터를 저차원의 데이터로 변환하는 방법
    - 기존의 변수들을 통해 새로운 변수를 추출하되, 기존 변수들의 분포특성을 최대한 보존하여(분산을 최대화하여) 결과의 신뢰성을 확보한다.   
    - ```python
      from sklearn.decomposition import PCA

      # PCA 적용: 차원을 2개로 축소
      pca = PCA(n_components=2)
      pca_df = pca.fit_transform(X)
      ```   

  - 요인분석(FA) : 데이터 안에 관찰할 수 없는 잠재 변수를 도출하고 데이터 내부의 구조를 해석하는 방법으로 사회과학이나 설문조사에서 많이 사용된다.
  - 선형판별분석(LDA) : 특정 공간 상에서 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간 분산과 클래스 내부 분산의 비율을 최대화 하는 방식으로 차원을 축소하는 기법
  - 다차원 척도법(MDS) : 개체들 사이의 유사성, 비유사성을 측정하여 2차원 또는 3차원 공간 상에 점으로표현하는 기법으로 개체들 사이의 집단화를 시각적으로 표현하는 기법
  - 특이값 분해(SVD) : M x N 차원의 행렬 데이터에서 특이값을 추출하고, 이를 통해 주어진 데이터세트를 효과적으로 축약할 수 있는 기법  


- **특징(변수) 제거 선택** : 중요하지 않거나 반복되는 특성을 제거하여 데이터의 성능을 최적화
  - 필터 방법
    - 상관계수 : 각 변수 간 상관관계를 계산하여 상관계수가 높은 변수들만 선택하는 방법. 주로 피어슨 상관계수를 사용함.
    - 카이제곱 검정 : 범주형 데이터간 독립성을 검정하여 상관성이 높은 변수들만 선택하는 방법. 주로 분류 문제에서 활용함.
    - 분산 분석 : 각 변수간 분산을 비교하여 설명력이 높은 변수들만 선택하는 방법
  - 랩퍼 방법
    - 전진 선택법 : 최적의 특성 집합을 찾는 방법으로 변수를 하나도 선택하지 않고 시작하여 성능이 가장 많이 향상되는 변수를 순차적으로 추가하는 방법
    - 후진 제거법 : 모든 특성(변수)을 포함한 상태에서 시작하여 성능에 가장 적은 영향을 미치는 특성들을 제거하는 방법
  - 임베디드 방법
    - 라쏘회귀 : L1 정규화를 적용하여 불필요한 특성의 계수를 0으로 만들어 제거하는 방법
    - 릿지회귀 : L2 정규화를 적용하여 특성들의 계수를 작게 만들어 과적합을 방지하는 방법
    - 엘라스틱넷 : L1과 L2 정규화를 결합하여 라쏘회귀와 릿지회귀의 장점을 결합한 방법




## 고유값(eigenvalue)과 고유벡터(eigenvector)에 대해 설명해 주세요. 이들이 데이터 분석에서 왜 중요한지 구체적인 예를 들어 설명해 주세요
---

> 고유값과 고유벡터는 선형대수학에 중요한 개념중 하나로 머신러닝에서 사용된다.

- **고유값과 고유벡터** : 선형 변환을 정의하는 정방행렬 A가 있을 때, 크기만 변하고 방향은 변하지 않는 0이 아닌 벡터를 `고유벡터`라고하며, 이 벡터가 변환된 후의 크기를 `고유값`이라고 한다.
  - 선형 변환 : 두 벡터를 더하거나 스칼라 값을 곱하는 연산

- **중요성** : 
  - 고유값과 고유벡터는 PCA(주성분 분석)에서 데이터의 분산을 설명하는데 사용된다 
  - 고유값이 큰 고유벡터는 데이터의 주요 변동 방향을 나타내며, 차원을 줄일 때 중요한 축이된다
  - 고유값은 각 고유벡터가 데이터의 분산을 얼마나 잘 설명하는지를 나타내며, 고유값이 클수록 그 방향이 데이터의 중요한 정보를 많이 포함하고 있음을 의미한다


## 히스토그램의 주요 단점은 무엇이며, 이를 극복하기 위한 대안적인 시각화 방법을 설명해 주세요
---

> 히스토그램은 데이터의 분포를 시각적으로 표현하는 그래프이다. 데이터를 일정한 구간(bin)으로 나누고, 각 구간에 해당하는 데이터의 빈도를 막대 그래프로 나타낸다. 이를 통해 데이터가 어떻게 분포되어 있는지, 어떤 값들이 자주 등장하는지를 직관적으로 파악할 수 있다.

- **히스토그램의 주요단점**
  1. 구간 선택의 민감성 : 빈(bin)의 크기와 개수에 따라 히스토그램의 모양이 달라질 수 있다. 빈 크기를 잘못설정할경우 데이터분포가 왜곡될 위험이 있다.
  2. 데이터 손실 : 히스토그램은 연속적인 데이터를 범주화하므로, 분포를 부드럽게 연결하지 못해 데이터의 세부적인 정보가 손실될 수 있다. 데이터를 세밀하게 시각화하기에 어려움이 있다.
  3. 비교의 어려움 : 히스토그램은 하나의 데이터세트를 시각화 하는데 적합하지만, 여러 데이터세트를 비교하여 시각화 할 때 복잡하여 해석이 어려울 수 있다.


- **대안적인 시각화 방안**
  - **커널 밀도 추정(KDE)** : 
    - KDE는 데이터의 밀도 함수를 추정하여 연속적인 곡선으로 시각화한다. 
    - 이를통해 데이터의 분포를 부드러운 곡선으로 나타내어, 구간설정(빈크기)에 민감하지 않고 정보 손실을 최소화하여 데이터의 연속적인 변화를 표현할 수 있다.
  - **누적 분포 함수(CDF)** : CDF는 주어진 값 이하의 데이터를 포함하는 누적 확률을 나타내어, 데이터의 전반적인 분포를 쉽게 파악할 수 있다.
  - **상자 그림(Box Plot)** : 박스플롯은 데이터의 사분위수와 이상치를 시각화하여 분포의 중심과 변동성을 보여줌으로써 여러 데이터 집합을 비교할 때 유용하다. 
  - **바이올린 플롯(Violin Plot)** : 
    - 바이올린 플롯은 히스토그램과 커널 밀도 추정(KDE)의 결합체라고 생각할 수 있다. 
    - 데이터의 분포를 부드러운 곡선(밀도)으로 나타내면서도, 데이터의 범위와 밀집도를 시각적으로 표현한다. 
    - 중앙값과 사분위수도 함께 표시하여, 데이터의 분포와 주요 통계치를 한눈에 파악할 수 있다.